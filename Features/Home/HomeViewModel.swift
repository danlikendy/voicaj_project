import Foundation
import Combine
import SwiftUI
import AVFoundation
import Speech

class HomeViewModel: ObservableObject {
    @Published var tasks: [TaskItem] = []
    @Published var collapsedSections: Set<TaskStatus> = []
    @Published var isRecordingButtonPulsing = true
    
    // MARK: - Recording States
    @Published var isRecording = false
    @Published var recordingTime: TimeInterval = 0
    @Published var recordingStartTime: Date?
    @Published var waveHeights: [CGFloat] = Array(repeating: 0.3, count: 20)
    
    private var cancellables = Set<AnyCancellable>()
    private var recordingTimer: Timer?
    private var audioRecorder: AVAudioRecorder?
    private var speechRecognizer: SFSpeechRecognizer?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var audioEngine: AVAudioEngine?
    
    init() {
        setupData()
        startPulsingAnimation()
        setupSpeechRecognition()
    }
    
    // MARK: - Speech Recognition Setup
    private func setupSpeechRecognition() {
        speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "ru-RU"))
        // –ù–µ —Å–æ–∑–¥–∞–µ–º audioEngine –∑–¥–µ—Å—å - —Å–æ–∑–¥–∞–¥–∏–º —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è
    }
    
    // MARK: - Recording Methods
    func startRecording() {
        guard !isRecording else { return }
        
        print("üé§ –ó–∞–ø—É—Å–∫ –∑–∞–ø–∏—Å–∏...")
        
        // –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω
        requestMicrophonePermission { [weak self] granted in
            guard let self = self else { return }
            
            print("üé§ –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω: \(granted)")
            
            if granted {
                // –ó–∞—Ç–µ–º –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
                self.requestSpeechRecognitionPermission { speechGranted in
                    print("üéØ –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏: \(speechGranted)")
                    
                    DispatchQueue.main.async {
                        if speechGranted {
                            // –°–æ–∑–¥–∞–µ–º audioEngine —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–π
                            self.audioEngine = AVAudioEngine()
                            
                            // –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞—É–¥–∏–æ—Å–µ—Å—Å–∏—é —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–π
                            self.setupAudioSession { success in
                                if success {
                                    print("‚úÖ –ê—É–¥–∏–æ—Å–µ—Å—Å–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                                    self.isRecording = true
                                    self.recordingStartTime = Date()
                                    self.recordingTime = 0
                                    self.startRecordingTimer()
                                    self.setupWaveAnimation()
                                    self.startAudioRecording()
                                    self.startSpeechRecognition()
                                } else {
                                    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∞—É–¥–∏–æ—Å–µ—Å—Å–∏—é")
                                }
                            }
                        } else {
                            print("‚ùå –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –Ω–µ –ø–æ–ª—É—á–µ–Ω–æ")
                        }
                    }
                }
            } else {
                DispatchQueue.main.async {
                    print("‚ùå –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω –Ω–µ –ø–æ–ª—É—á–µ–Ω–æ")
                }
            }
        }
    }
    
    func stopRecording() {
        guard isRecording else { return }
        
        DispatchQueue.main.async {
            self.isRecording = false
            self.stopRecordingTimer()
            self.stopAudioRecording()
            self.stopSpeechRecognition()
            
            // –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø–∏—Å–∏
            self.createTaskFromRecording()
        }
    }
    
    private func startRecordingTimer() {
        recordingTimer = Timer.scheduledTimer(withTimeInterval: 0.1, repeats: true) { [weak self] _ in
            Task { @MainActor in
                guard let self = self, let startTime = self.recordingStartTime else { return }
                self.recordingTime = Date().timeIntervalSince(startTime)
            }
        }
    }
    
    private func stopRecordingTimer() {
        recordingTimer?.invalidate()
        recordingTimer = nil
    }
    
    private func setupWaveAnimation() {
        Timer.scheduledTimer(withTimeInterval: 0.1, repeats: true) { [weak self] _ in
            Task { @MainActor in
                guard let self = self else { return }
                for i in 0..<self.waveHeights.count {
                    self.waveHeights[i] = CGFloat.random(in: 0.1...1.0)
                }
            }
        }
    }
    
    private func requestMicrophonePermission(completion: @escaping (Bool) -> Void) {
        if #available(iOS 17.0, *) {
            // –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π API –¥–ª—è iOS 17.0+
            AVAudioApplication.requestRecordPermission { granted in
                DispatchQueue.main.async {
                    print("üé§ –†–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–ø—Ä–æ—Å–∞ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω (iOS 17+): \(granted)")
                    completion(granted)
                }
            }
        } else {
            // Fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π iOS
            let audioSession = AVAudioSession.sharedInstance()
            let currentStatus = audioSession.recordPermission
            
            print("üîç –¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω (legacy): \(currentStatus.rawValue)")
            
            switch currentStatus {
            case .granted:
                print("‚úÖ –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω —É–∂–µ –ø–æ–ª—É—á–µ–Ω–æ")
                completion(true)
            case .denied:
                print("‚ùå –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω –æ—Ç–∫–ª–æ–Ω–µ–Ω–æ")
                completion(false)
            case .undetermined:
                print("‚ùì –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ, –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º...")
                audioSession.requestRecordPermission { granted in
                    DispatchQueue.main.async {
                        print("üé§ –†–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–ø—Ä–æ—Å–∞ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω: \(granted)")
                        completion(granted)
                    }
                }
            @unknown default:
                print("‚ùì –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å—Ç–∞—Ç—É—Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω")
                completion(false)
            }
        }
    }
    
    private func requestSpeechRecognitionPermission(completion: @escaping (Bool) -> Void) {
        SFSpeechRecognizer.requestAuthorization { status in
            DispatchQueue.main.async {
                let granted = status == .authorized
                completion(granted)
            }
        }
    }
    
    private func setupAudioSession(completion: @escaping (Bool) -> Void) {
        let audioSession = AVAudioSession.sharedInstance()
        
        do {
            print("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—É–¥–∏–æ—Å–µ—Å—Å–∏–∏...")
            
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π API –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            if #available(iOS 17.0, *) {
                print("üîß –°—Ç–∞—Ç—É—Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ –∑–∞–ø–∏—Å—å (iOS 17+): –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π API")
            } else {
                let permissionStatus = audioSession.recordPermission
                print("üîß –°—Ç–∞—Ç—É—Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ –∑–∞–ø–∏—Å—å (legacy): \(permissionStatus.rawValue)")
            }
            
            // –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞—É–¥–∏–æ—Å–µ—Å—Å–∏—é –¥–ª—è –∑–∞–ø–∏—Å–∏
            try audioSession.setCategory(.playAndRecord, mode: .default, options: [.defaultToSpeaker])
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            
            print("‚úÖ –ê—É–¥–∏–æ—Å–µ—Å—Å–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            completion(true)
        } catch {
            print("‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∞—É–¥–∏–æ—Å–µ—Å—Å–∏–∏: \(error)")
            completion(false)
        }
    }
    
    private func startAudioRecording() {
        do {
            let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
            let audioFilename = documentsPath.appendingPathComponent("recording_\(Date().timeIntervalSince1970).m4a")
            
            let settings = [
                AVFormatIDKey: Int(kAudioFormatMPEG4AAC),
                AVSampleRateKey: 44100,
                AVNumberOfChannelsKey: 1,
                AVEncoderAudioQualityKey: AVAudioQuality.high.rawValue
            ]
            
            audioRecorder = try AVAudioRecorder(url: audioFilename, settings: settings)
            audioRecorder?.record()
            
            print("üé§ –ó–∞–ø–∏—Å—å –∞—É–¥–∏–æ –Ω–∞—á–∞—Ç–∞")
            
        } catch {
            print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –∞—É–¥–∏–æ: \(error)")
        }
    }
    
    private func stopAudioRecording() {
        audioRecorder?.stop()
        audioRecorder = nil
    }
    
    private func startSpeechRecognition() {
        guard let audioEngine = audioEngine, let speechRecognizer = speechRecognizer else { 
            print("‚ùå –ê—É–¥–∏–æ –¥–≤–∏–∂–æ–∫ –∏–ª–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å —Ä–µ—á–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return 
        }
        
        if audioEngine.isRunning {
            audioEngine.stop()
            recognitionRequest?.endAudio()
            return
        }
        
        do {
            recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
            
            guard let recognitionRequest = recognitionRequest else { 
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∑–∞–ø—Ä–æ—Å –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ")
                return 
            }
            recognitionRequest.shouldReportPartialResults = true
            
            let inputNode = audioEngine.inputNode
            let recordingFormat = inputNode.outputFormat(forBus: 0)
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
                recognitionRequest.append(buffer)
            }
            
            audioEngine.prepare()
            try audioEngine.start()
            
            print("üéØ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∑–∞–ø—É—â–µ–Ω–æ")
            
            recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
                if let result = result {
                    let transcript = result.bestTranscription.formattedString
                    print("üìù –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è: \(transcript)")
                }
                
                if error != nil || result?.isFinal == true {
                    self?.audioEngine?.stop()
                    inputNode.removeTap(onBus: 0)
                    self?.recognitionRequest = nil
                    self?.recognitionTask = nil
                    print("üõë –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
                }
            }
            
        } catch {
            print("‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏: \(error)")
        }
    }
    
    private func stopSpeechRecognition() {
        audioEngine?.stop()
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
    }
    
    private func createTaskFromRecording() {
        let newTask = TaskItem(
            id: UUID(),
            title: "–ó–∞–ø–∏—Å—å –æ—Ç \(DateFormatter.localizedString(from: Date(), dateStyle: .short, timeStyle: .short))",
            description: "–ì–æ–ª–æ—Å–æ–≤–∞—è –∑–∞–ø–∏—Å—å –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é \(Int(recordingTime)) —Å–µ–∫—É–Ω–¥",
            status: .planned,
            priority: .medium,
            dueDate: nil,
            tags: ["–≥–æ–ª–æ—Å–æ–≤–∞—è-–∑–∞–ø–∏—Å—å"],
            isPrivate: false,
            audioURL: nil,
            transcript: nil,
            createdAt: Date(),
            updatedAt: Date(),
            completedAt: nil,
            parentTaskId: nil,
            subtasks: []
        )
        
        tasks.append(newTask)
        print("‚úÖ –°–æ–∑–¥–∞–Ω–∞ –∑–∞–¥–∞—á–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø–∏—Å–∏")
    }
    
    // MARK: - Computed Properties
    
    var currentDateString: String {
        let formatter = DateFormatter()
        formatter.dateFormat = "E, d MMMM"
        formatter.locale = Locale(identifier: "ru_RU")
        return formatter.string(from: Date())
    }
    
    var currentDayString: String {
        let formatter = DateFormatter()
        formatter.dateFormat = "d MMMM"
        formatter.locale = Locale(identifier: "ru_RU")
        return formatter.string(from: Date())
    }
    
    var greeting: String {
        let hour = Calendar.current.component(.hour, from: Date())
        
        switch hour {
        case 5..<12:
            return "–î–æ–±—Ä–æ–µ —É—Ç—Ä–æ"
        case 12..<17:
            return "–î–æ–±—Ä—ã–π –¥–µ–Ω—å"
        case 17..<22:
            return "–î–æ–±—Ä—ã–π –≤–µ—á–µ—Ä"
        default:
            return "–î–æ–±—Ä–æ–π –Ω–æ—á–∏"
        }
    }
    
    var currentStreak: Int {
        // TODO: Implement streak calculation based on voice records
        return 7
    }
    
    var recordingButtonText: String {
        let hour = Calendar.current.component(.hour, from: Date())
        
        if hour < 12 {
            return "–°–¥–µ–ª–∞–π—Ç–µ –ø–ª–∞–Ω –Ω–∞ —Å–µ–≥–æ–¥–Ω—è"
        } else {
            return "–ó–∞–ø–∏—à–∏—Ç–µ –∏—Ç–æ–≥ –¥–Ω—è"
        }
    }
    
    var availableTags: [String] {
        let allTags = tasks.flatMap { $0.tags }
        return Array(Set(allTags)).sorted()
    }
    
    // MARK: - Public Methods
    
    func tasksForStatus(_ status: TaskStatus) -> [TaskItem] {
        return tasks.filter { $0.status == status }
    }
    
    func toggleSection(_ status: TaskStatus) {
        if collapsedSections.contains(status) {
            collapsedSections.remove(status)
        } else {
            collapsedSections.insert(status)
        }
    }
    
    func completeTask(_ task: TaskItem) {
        if let index = tasks.firstIndex(where: { $0.id == task.id }) {
            tasks[index].status = .completed
            tasks[index].completedAt = Date()
            tasks[index].updatedAt = Date()
        }
    }
    
    func snoozeTask(_ task: TaskItem, until date: Date) {
        if let index = tasks.firstIndex(where: { $0.id == task.id }) {
            tasks[index].dueDate = date
            tasks[index].updatedAt = Date()
        }
    }
    
    func deleteTask(_ task: TaskItem) {
        tasks.removeAll { $0.id == task.id }
    }
    
    // MARK: - Private Methods
    
    private func setupData() {
        // TODO: Load tasks from data service
        loadSampleData()
    }
    
    private func startPulsingAnimation() {
        Timer.publish(every: 3.0, on: .main, in: .common)
            .autoconnect()
            .sink { [weak self] _ in
                withAnimation(.easeInOut(duration: 0.5)) {
                    self?.isRecordingButtonPulsing.toggle()
                }
            }
            .store(in: &cancellables)
    }
    
    private func loadSampleData() {
        // Sample tasks for development
        let sampleTasks = [
            TaskItem(
                title: "–°–¥–µ–ª–∞—Ç—å —É–±–æ—Ä–∫—É",
                description: "–ü—Ä–æ–ø—ã–ª–µ—Å–æ—Å–∏—Ç—å –∏ –ø–æ–º—ã—Ç—å –ø–æ–ª—ã",
                status: .completed,
                priority: .medium,
                tags: ["–¥–æ–º", "–±—ã—Ç"],
                createdAt: Date().addingTimeInterval(-86400)
            ),
            TaskItem(
                title: "–ö—É–ø–∏—Ç—å –ø—Ä–æ–¥—É–∫—Ç—ã",
                description: "–ú–æ–ª–æ–∫–æ, —Ö–ª–µ–±, —è–π—Ü–∞",
                status: .planned,
                priority: .high,
                dueDate: Date().addingTimeInterval(3600),
                tags: ["–ø–æ–∫—É–ø–∫–∏", "–±—ã—Ç"]
            ),
            TaskItem(
                title: "–ü–æ–∑–≤–æ–Ω–∏—Ç—å –º–∞–º–µ",
                description: "–£–∑–Ω–∞—Ç—å –∫–∞–∫ –¥–µ–ª–∞",
                status: .important,
                priority: .high,
                tags: ["—Å–µ–º—å—è", "–∑–≤–æ–Ω–∫–∏"]
            ),
            TaskItem(
                title: "–ó–∞–ø–∏—Å–∞—Ç—å—Å—è –∫ –≤—Ä–∞—á—É",
                description: "–¢–µ—Ä–∞–ø–µ–≤—Ç, –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π –Ω–µ–¥–µ–ª–µ",
                status: .stuck,
                priority: .medium,
                tags: ["–∑–¥–æ—Ä–æ–≤—å–µ", "–≤—Ä–∞—á"]
            ),
            TaskItem(
                title: "–ò–∑—É—á–∏—Ç—å SwiftUI",
                description: "–ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ iOS 17",
                status: .idea,
                priority: .low,
                tags: ["—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞", "–æ–±—É—á–µ–Ω–∏–µ"]
            )
        ]
        
        tasks = sampleTasks
    }
}
